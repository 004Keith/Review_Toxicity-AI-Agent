# AI Agent for Toxicity Detection in Logs

This project leverages a Hugging Face transformers zero-shot classifier to detect toxic or profane language in log data.

## ğŸš€ Features

- Zero-shot classification with Hugging Face transformers
- Profanity and toxicity detection in logs
- Customizable with Kaggle datasets via the `datasets` library

## ğŸ“ Files

- `ai_agent_toxicity_detection.ipynb`: Main notebook with data loading, preprocessing, model inference, and evaluations.
- `requirements.txt`: Python dependencies.

## ğŸ§ª Setup

```bash
git clone https://github.com/YOUR-USERNAME/ai-agent-toxicity
cd ai-agent-toxicity
pip install -r requirements.txt
jupyter notebook ai_agent_toxicity_detection.ipynb
```

## ğŸ§  Dependencies

- transformers
- datasets
- pandas
- torch

## ğŸ›  Future Enhancements

- Fine-tune a local transformer model for higher accuracy  
- Integrate into a real-time log monitoring pipeline  
- Expand to cover additional threat categories

## ğŸ§‘â€ğŸ’» Author

- Built as part of an AI & Security exploration project.
- Keith Warren Mathias - https://github.com/004Keith
- Prathvi G Shetty - https://github.com/BlunderingProgrammmer
  

## ğŸ“œ License

MIT License
